{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "#### Extracting linguistic features using ```spaCy```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_661/1116081116.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# import modules \n",
    "import spacy\n",
    "import pandas as pd \n",
    "import os\n",
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "# loading the spacy model\n",
    "# define pipeline\n",
    "nlp = spacy.load(\"en_core_web_md\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ```Preprocessing``` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at the dataframe it seems the text has some spaces enoced as \"\\n\". We want to replace these with spaces. \n",
    "# we also want to get rid of the doc.id and title between the brackets \n",
    "\n",
    "# make function to clean the text\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    This function removes all non-alphanumeric characters (\\W+), extra white spaces \"\\n\" and characters between < and > (.*?), and trailing whitespaces (.strip). \n",
    "    Next it makes all characters lowercase. \n",
    "    \n",
    "    \"\"\"\n",
    "    cleaned_text = re.sub(r'<.*?>|\\n|\\W+', ' ', text).strip().lower()\n",
    "\n",
    "    return cleaned_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get relative frequency of ```Nouns, Verbs, Adjective, and Adverbs``` per 10,000 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "def rel_freq(doc):\n",
    "    \"\"\"\n",
    "    This function calculates the relative frequencies of Nous, Verbs Adjectives and Adverbs the each text.  \n",
    "    \n",
    "    \"\"\"\n",
    "  # Total word count in the document\n",
    "  total_word_count = len(doc)\n",
    "\n",
    "  # make dictionary \n",
    "  counts_per_pos = {\"NOUN\": 0, \"ADV\": 0, \"ADJ\": 0, \"VERB\": 0} \n",
    "\n",
    "    # Count occurrences of each POS\n",
    "  for token in doc: \n",
    "      if token.pos_ in counts_per_pos: # only take NOUN, ADV, ADj and VERB\n",
    "              # get text and label \n",
    "          counts_per_pos[token.pos_] += 1\n",
    "        \n",
    "     # Calculate relative frequencies per 10,000 words\n",
    "      relative_frequencies_per_10000 = {} # initialize empty dictionary \n",
    "      for pos, count in counts_per_pos.items(): # iterate through the dictionary and make the values relative \n",
    "          relative_frequency = (count / total_word_count) * 10000\n",
    "          relative_frequencies_per_10000[pos] = relative_frequency\n",
    "\n",
    "  return relative_frequencies_per_10000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get total number of *unique* PER, LOC, ORGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def unique_ents(doc):\n",
    "    \"\"\"\n",
    "    This function calculates the amount of different people, locations and organizations mentioned in the text.  \n",
    "    \n",
    "    \"\"\"\n",
    "    counts_per_label = {\"PERSON\": set(), \"LOC\": set(), \"ORG\": set()}\n",
    "\n",
    "    seen_entities = set()  # Keep track of seen entities in each document\n",
    "    for entity in doc.ents:\n",
    "        if entity.label_ in counts_per_label and entity.text not in seen_entities:\n",
    "            # Increment the count for each label\n",
    "            counts_per_label[entity.label_].add(entity.text)\n",
    "            seen_entities.add(entity.text)  # Add the entity to seen_entities set\n",
    "\n",
    "    # After processing all documents, convert sets to counts\n",
    "    counts_per_label = {label: len(entities) for label, entities in counts_per_label.items()}\n",
    "\n",
    "    return counts_per_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop over each text file in the folder called ```in```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def processing(path):    \n",
    "\n",
    "    \"\"\"\n",
    "    This function loads in the data. Then it applies the clean_text function to the raw text before converting it into a spaCy doc.\n",
    "    Next it applies the two functions: rel_freq() and unique_ents() to extract the linguistic features. \n",
    "    \"\"\"\n",
    "    # open it and read it \n",
    "    with open(path, encoding=\"latin-1\") as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    text = clean_text(text)\n",
    "    doc = nlp(text)\n",
    "\n",
    "    relative_frequencies_per_10000 = rel_freq(doc) \n",
    "    counts_per_label = unique_ents(doc)\n",
    "\n",
    "    data = relative_frequencies_per_10000 | counts_per_label  # merge the two dictionaries \n",
    "\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each sub-folder (a1, a2, a3, ...) save a table which shows the following information:\n",
    "| Filename  | RelFreq NOUN | RelFreq VERB | RelFreq ADJ | RelFreq ADV | Unique PER | Unique LOC | Unique ORG |\n",
    "|-----------|--------------|--------------|-------------|-------------|------------|------------|------------|\n",
    "| file1.txt | ---          | ---          | ---         | ---         | ---        | ---        | ---        |\n",
    "| file2.txt | ---          | ---          | ---         | ---         | ---        | ---        | ---        |\n",
    "| etc       | ---          | ---          | ---         | ---         | ---        | ---        | ---        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/work/CDS-language/CDS-language/Assignments/Assignment1_lang/src'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get current working directory \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define paths\n",
    "file_path = os.path.join(\n",
    "    \"..\", \n",
    "    \"in\",\n",
    "    \"USEcorpus\")\n",
    "\n",
    "output_path = os.path.join(\n",
    "    \"..\", \n",
    "    \"out\")\n",
    "\n",
    "dirs = sorted(os.listdir(file_path))\n",
    "\n",
    "\n",
    "# loop through the directories \n",
    "for directory in dirs: \n",
    "    subfolder = os.path.join(file_path, directory) # path.join instead of \"datapath + \"/\" + directory\"\n",
    "    filenames = sorted(os.listdir(subfolder))\n",
    "\n",
    "    corpus_texts = [] # make empty list to append the texts \n",
    "\n",
    "    for text_file in filenames: # loop through the files\n",
    "        path = os.path.join(subfolder,text_file)\n",
    "        data = processing(path)\n",
    "        \n",
    "        corpus_texts.append({'File': text_file, 'Folder': directory, **data}) # use dictionary, so it's easier to convert to df with the folder and file name\n",
    "\n",
    "\n",
    "        # Convert the list of dictionaries to a pandas DataFrame\n",
    "    corpus_df = pd.DataFrame(corpus_texts)\n",
    "    #rename the columns\n",
    "    corpus_df = corpus_df.rename({'PERSON': 'Unique PER', \n",
    "                      'LOC': 'Unique LOC', \n",
    "                      'ORG': 'Unique ORG', \n",
    "                      'NOUN': 'RelFreq NOUN', \n",
    "                      'ADV': 'RelFreq ADV', \n",
    "                      'ADJ': 'RelFreq ADJ', \n",
    "                      'VERB': 'RelFreq VERB'}, axis='columns')\n",
    "    # save in the folders with the tables \n",
    "    corpus_df.to_csv(os.path.join(output_path, f\"{directory}_spacy.csv\"), index = False)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
