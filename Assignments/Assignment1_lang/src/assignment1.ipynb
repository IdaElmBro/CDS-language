{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "#### Extracting linguistic features using ```spaCy```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_967/1459457600.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "# import modules \n",
    "import spacy\n",
    "import pandas as pd \n",
    "import os\n",
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "# loading the spacy model\n",
    "# define pipeline\n",
    "nlp = spacy.load(\"en_core_web_md\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ```Preprocessing``` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at the dataframe it seems the text has some spaces enoced as \"\\n\". We want to replace these with spaces. \n",
    "# we also want to get rid of the doc.id and title between the brackets \n",
    "\n",
    "# make function to clean the text\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    This function removes extra white spaces \"\\n\" and characters between < and >, and trailing whitespaces (.strip). \n",
    "    \"\"\"\n",
    "    cleaned_text = re.sub(r'<.*?>|\\s+', ' ', text).strip()\n",
    "\n",
    "    return cleaned_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get relative frequency of ```Nouns, Verbs, Adjective, and Adverbs``` per 10,000 words\n",
    "### And get total number of *unique* PER, LOC, ORGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# one long function \n",
    "\n",
    "def processing(path):    \n",
    "    # open it and read it \n",
    "    with open(path, encoding=\"latin-1\") as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    doc = nlp(text)\n",
    "\n",
    "    #doc = clean_text(doc)\n",
    "\n",
    "    # see some attributes \n",
    "    counts_per_pos = {\"NOUN\": 0, \"ADV\": 0, \"ADJ\": 0, \"VERB\": 0} # make dictionary \n",
    "\n",
    "    for token in doc: \n",
    "        if token.pos_ in counts_per_pos: # only take NOUN, ADV, ADj and VERB\n",
    "                # get text and label \n",
    "            counts_per_pos[token.pos_] += 1\n",
    "\n",
    "\n",
    "    counts_per_label = {\"PERSON\": 0, \"LOC\": 0, \"ORG\": 0}\n",
    "\n",
    "    # docs is a list of spaCy Doc objects\n",
    "    for entity in doc.ents:\n",
    "        if entity.label_ in counts_per_label:\n",
    "            # Increment the count for each label\n",
    "            counts_per_label[entity.label_] += 1\n",
    "\n",
    "    \n",
    "    data = counts_per_label | counts_per_pos # merge the two dictionaries \n",
    "        \n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "#make relative...; \n",
    "#  counts = df[\"pos\"].value_counts()\n",
    "#                relative_freqs_per_10000 = (counts / len(df)) * 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop over each text file in the folder called ```in```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/work/CDS-language/CDS-language/Assignments/Assignment1_lang/src'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get current working directory \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define paths\n",
    "file_path = os.path.join(\n",
    "    \"..\", \n",
    "    \"in\",\n",
    "    \"USEcorpus\")\n",
    "\n",
    "output_path = os.path.join(\n",
    "    \"..\", \n",
    "    \"out\")\n",
    "\n",
    "dirs = sorted(os.listdir(file_path))\n",
    "\n",
    "\n",
    "# loop through the paths \n",
    "for directory in dirs: \n",
    "    subfolder = os.path.join(file_path, directory) # path.join instead of \"datapath + \"/\" + directory\"\n",
    "    filenames = sorted(os.listdir(subfolder))\n",
    "\n",
    "    corpus_texts = [] # make empty list to append the texts \n",
    "\n",
    "    for text_file in filenames:\n",
    "        path = os.path.join(subfolder,text_file)\n",
    "        data = processing(path)\n",
    "        \n",
    "        corpus_texts.append({'File': text_file, 'Folder': directory, **data}) # use dictionary, so it's easier to convert to df with the folder and file name\n",
    "\n",
    "\n",
    "        # Convert the list of dictionaries to a pandas DataFrame\n",
    "    corpus_df = pd.DataFrame(corpus_texts)\n",
    "    corpus_df.to_csv(os.path.join(output_path, f\"{directory}_spacy.csv\"), index = False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:153\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:182\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/work/CDS-language/CDS-language/Assignments/Assignment1_lang/src/assignment1.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://app-5019744-0.cloud.sdu.dk/work/CDS-language/CDS-language/Assignments/Assignment1_lang/src/assignment1.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m corpus_df[\u001b[39m0\u001b[39;49m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:4090\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4088\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   4089\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4090\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   4091\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4092\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3809\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(casted_key, \u001b[39mslice\u001b[39m) \u001b[39mor\u001b[39;00m (\n\u001b[1;32m   3805\u001b[0m         \u001b[39misinstance\u001b[39m(casted_key, abc\u001b[39m.\u001b[39mIterable)\n\u001b[1;32m   3806\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39many\u001b[39m(\u001b[39misinstance\u001b[39m(x, \u001b[39mslice\u001b[39m) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m casted_key)\n\u001b[1;32m   3807\u001b[0m     ):\n\u001b[1;32m   3808\u001b[0m         \u001b[39mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3809\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3810\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3811\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3812\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3814\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ```Preprocessing``` functions \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each sub-folder (a1, a2, a3, ...) save a table which shows the following information:\n",
    "| Filename  | RelFreq NOUN | RelFreq VERB | RelFreq ADJ | RelFreq ADV | Unique PER | Unique LOC | Unique ORG |\n",
    "|-----------|--------------|--------------|-------------|-------------|------------|------------|------------|\n",
    "| file1.txt | ---          | ---          | ---         | ---         | ---        | ---        | ---        |\n",
    "| file2.txt | ---          | ---          | ---         | ---         | ---        | ---        | ---        |\n",
    "| etc       | ---          | ---          | ---         | ---         | ---        | ---        | ---        |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
